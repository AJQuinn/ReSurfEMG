{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning\n",
    "This notebook is under development- please use to evaluate entropy notebook and siggest desired changes only. Debugged 30/5/2022, new review pending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makeda\\anaconda3\\envs\\emgandash\\lib\\site-packages\\mne\\fixes.py:321: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(scipy.__version__) >= '1.1':\n",
      "C:\\Users\\makeda\\anaconda3\\envs\\emgandash\\lib\\site-packages\\mne\\fixes.py:1134: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numba.__version__) < LooseVersion('0.40'):\n",
      "C:\\Users\\makeda\\anaconda3\\envs\\emgandash\\lib\\site-packages\\mne\\fixes.py:1134: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numba.__version__) < LooseVersion('0.40'):\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from TMSiSDK.file_readers import Poly5Reader\n",
    "import collections\n",
    "import math\n",
    "sys.path.insert(0, '../resurfemg')\n",
    "import helper_functions as hf\n",
    "from config import Config\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a collection place for experiments and models (must be uncommented later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'finalized_model.sav'\n",
    "# joblib.dump(model, filename)\n",
    " \n",
    "# # some time later...\n",
    " \n",
    "# # load the model from disk\n",
    "# loaded_model = joblib.load(filename)\n",
    "# result = loaded_model.fit(X_test, Y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not rerun this cell\n",
    "big_data_list= []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below change the path to the root directory where you are keeping your EMGs and ventilator \"Draeger\" files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reruns should be done from this cell as the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_emg_directory = config.get_directory('root_emg_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_pattern = os.path.join(root_emg_directory, '**/*.Poly5')\n",
    "emg_and_draeger_files = glob.glob(emg_pattern, recursive=True)\n",
    "\n",
    "emg_files = []\n",
    "draeger_files = []\n",
    "\n",
    "for file in emg_and_draeger_files:\n",
    "    if 'Draeger' in file:\n",
    "        draeger_files.append(file)\n",
    "    else:\n",
    "        emg_files.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can pick a file from the list, which have been numbered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_numbers_strung = []\n",
    "for i in range(len(emg_files)):\n",
    "    list_of_numbers_strung.append(str(i))\n",
    "\n",
    "\n",
    "btn = widgets.Dropdown(\n",
    "    options=list_of_numbers_strung,\n",
    "    value='0',\n",
    "    description='Picked File:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(btn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_chosen = int(btn.value)\n",
    "file_chosen = emg_files[number_chosen] \n",
    "print(\"The file you chose is:\",file_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_emg = Poly5Reader(file_chosen)\n",
    "data_samples= data_emg.samples\n",
    "emg_sample_rate = data_emg.sample_rate\n",
    "converted_to_seconds =  []\n",
    "converted_to_samples = []\n",
    "for i in range(len(data_samples[0])):\n",
    "    converted_to_seconds.append(i/emg_sample_rate)\n",
    "    converted_to_samples.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# set up plotn\n",
    "x = data_samples\n",
    "fig, axis = plt.subplots(nrows = 3, ncols = 2, figsize=(6, 6))\n",
    "#ax.set_ylim([-4, 4])\n",
    "axis[0,0].grid(True)\n",
    "axis[0,0].plot(x[0])\n",
    "axis[0,0].set(title='leads in samples')\n",
    "axis[1,0].plot(x[1])\n",
    "axis[2,0].plot(x[2])\n",
    "axis[0,1].set(title='leads in seconds')\n",
    "axis[0,1].grid(True)\n",
    "axis[0,1].plot(converted_to_seconds,x[0])\n",
    "axis[1,1].plot(converted_to_seconds,x[1])\n",
    "axis[2,1].plot(converted_to_seconds,x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the whole unfiltered EMG, but you probably want to examine a part. You will also want to examine something filtered down to only the EMG components. Therefore we will filter off only the EMG components with an ICA in addtion to the filter we will play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can filter down to which part you want to see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to cut and see the file in samples or seconds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis = widgets.Dropdown(\n",
    "    options=[\"Samples\",\"Seconds\"],\n",
    "    value='Samples',\n",
    "    description=\"Select View Option\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(y_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_view= y_axis.value\n",
    "time_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type in start number and press return for it to update\n",
    "start = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type in end number and press return for it to update\n",
    "end= input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we want to?\n",
    "# Add warning cell if end is less than start, or numbers are out of range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_s = float(start)* emg_sample_rate\n",
    "end_s = float(end)*emg_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_view == 'Samples':\n",
    "    # nox examine at a certain scale- from point a to b as samples\n",
    "    x = data_samples\n",
    "    fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "    ax_1.grid(True)\n",
    "    ax_1.plot(x[0][int(start):int(end)])\n",
    "    ax_1.set(title='leads, samples')\n",
    "    ax_2.plot(x[1][int(start):int(end)])\n",
    "    ax_3.plot(x[2][int(start):int(end)])\n",
    "    \n",
    "if time_view == 'Seconds':\n",
    "    # nox examine at a certain scale- from point a to b as samples\n",
    "    x_for_secs = data_samples\n",
    "    \n",
    "    converter_for_sample_number =90\n",
    "    \n",
    "    y = converted_to_seconds\n",
    "    fig, (ax_1,ax_2,ax_3) = plt.subplots(nrows = 3, figsize=(6, 4))\n",
    "    ax_1.grid(True)\n",
    "    ax_1.plot(y[int(start_s):int(end_s)],x[0][int(start_s):int(end_s)])\n",
    "    ax_1.set(title='leads, seconds')\n",
    "    ax_2.plot(y[int(start_s):int(end_s)],x[1][int(start_s):int(end_s)])\n",
    "    ax_3.plot(y[int(start_s):int(end_s)],x[2][int(start_s):int(end_s)])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy with your selection? If not redo, then we can see how the filter the selection in a basic pipleline before extracint entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def working_pipeline_pre_entropy(our_chosen_samples): \n",
    "    cut_file_data = hf.bad_end_cutter_for_samples(our_chosen_samples, percent_to_cut=3, tolerance_percent=5)\n",
    "    bd_filtered_file_data = hf.emg_bandpass_butter_sample(cut_file_data, 5, 450, 2048, output='sos')\n",
    "    # step 3 end-cutting again to get rid of filtering artifacts\n",
    "    re_cut_file_data = hf.bad_end_cutter_for_samples(bd_filtered_file_data, percent_to_cut=3, tolerance_percent=5)\n",
    "    # skip step4 and do step 5 ICA\n",
    "    components = hf.compute_ICA_two_comp(re_cut_file_data)\n",
    "    #     the picking step!\n",
    "    emg= hf.pick_more_peaks_array(components)\n",
    "    # now process it in final steps\n",
    "    abs_values = abs(emg)\n",
    "    final_envelope_d = hf.emg_highpass_butter(abs_values, 150, 2048)\n",
    "    \n",
    "        \n",
    "    return final_envelope_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_emg = working_pipeline_pre_entropy(data_samples)\n",
    "plt.plot(processed_data_emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we created some basic processed EMG. We will graph it based on te sample selected and the cutoff on entropy.\n",
    "\n",
    "TODO: add seconds option here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to select where the cut_off is\n",
    "\n",
    "entropy_cutoff = widgets.Dropdown(\n",
    "    options=[\"Mean\",\"Half_range\"],\n",
    "    value='Mean',\n",
    "    description=\"Select Entropy Cut off\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(entropy_cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_data_emg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_list = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,19,20,21,22,223,24,25,26,27,28,29,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give i to i+5\n",
    "# then i+5 to i+2x5\n",
    "# then i+ 2x5 to 3x5\n",
    "def jumpIterator(lst,sliceLen, jump):\n",
    "    for i in range(len(lst) - (sliceLen )):\n",
    "        yield lst[(jump*i): (( jump*i) + sliceLen)]\n",
    "index_ml_hold = []\n",
    "for slice in jumpIterator(toy_list, 5,3):\n",
    "    ml_index = slice\n",
    "    print(slice)\n",
    "    index_ml_hold.append(ml_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliceIterator(lst, sliceLen):\n",
    "    for i in range(len(lst) - (sliceLen )):\n",
    "        yield lst[i:i + sliceLen]\n",
    "index_ml_hold = []\n",
    "for slice in sliceIterator(toy_list, 5):\n",
    "    ml_index = slice\n",
    "    print(slice)\n",
    "    index_ml_hold.append(ml_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_hold_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded_for_ent(stralist):\n",
    "    rounded= np.round_(stralist, decimals = 5)\n",
    "    return rounded\n",
    "if time_view == 'Samples':\n",
    "    big_list = rounded_for_ent(processed_data_emg[int(start):int(end)])# replace with whole array of time series!\n",
    "else:\n",
    "    big_list = rounded_for_ent(processed_data_emg[int(start_s):int(end_s)])\n",
    "slice_length = 100\n",
    "def sliceIterator(lst, sliceLen):\n",
    "    for i in range(len(lst) - sliceLen + 1):\n",
    "        yield lst[i:i + sliceLen]\n",
    "index_hold = []\n",
    "for slice in sliceIterator(big_list, slice_length):\n",
    "    entropy_index = hf.entropical(slice)\n",
    "    index_hold.append(entropy_index)\n",
    "\n",
    "if entropy_cutoff.value == 'Half_range':\n",
    "    decision_cutoff = (np.max(index_hold) + np.min(index_hold))/2\n",
    "else:# entropy_cutoff.value == 'Mean':\n",
    "    decision_cutoff = np.mean(index_hold)\n",
    "\n",
    "\n",
    "rms_rolled = hf.vect_naive_rolling_rms(index_hold,100) # so rms is rms entropy\n",
    "if time_view == 'Samples':\n",
    "    #y= converted_to_samples\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],processed_data_emg[int(start):(int(start) + len(rms_rolled))]*1000)\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],rms_rolled)\n",
    "    plt.plot(converted_to_samples[int(start):(int(start) + len(rms_rolled))],hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff))\n",
    "    plt.axhline(y = decision_cutoff, color = 'r', linestyle = '-')\n",
    "    \n",
    "else:\n",
    "    y = converted_to_seconds\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))], processed_data_emg[int(start_s):(int(start_s)+len(rms_rolled))]*1000)\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))],rms_rolled)\n",
    "    plt.plot(converted_to_seconds[int(start_s):(int(start_s)+int(len(rms_rolled)))],hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff))\n",
    "    plt.axhline(y = decision_cutoff, color = 'r', linestyle = '-')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to do this adaptively over the whole signal. But how often should we cut to get the mean or half value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can compare counted 'breaths' to algorithm breaths, then we can compare a sequence of peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_count = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_view == 'Seconds':    \n",
    "     decision_line = hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff)\n",
    "else:\n",
    "    decision_line = hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff)\n",
    "    \n",
    "max_per_peak_count = hf.count_decision_array(decision_line)\n",
    "max_per_peak_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our peak value sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Also we are looking at distance from zero to positive maxima, but the amplitude would be from adjancent low value...\n",
    "We can look at an absolute value array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maxima_in_high_entropy_area(our_array,start=0, end=10000, decision_cutoff='mean'):\n",
    "    \"\"\"\n",
    "    Finds maxima in high entropy areas. You need to have made an rms rolled variable.\n",
    "    The function is not yet optimized, but works. \n",
    "    \"\"\"\n",
    "    # big_list = np.round(our_array[start:end], 5)\n",
    "    # np.round_(stralist, decimals = 5)\n",
    "    decision_array = hf.zero_one_for_jumps_base(rms_rolled,decision_cutoff)\n",
    "    if decision_array[0] == 1:\n",
    "        ups_and_downs = np.logical_xor(decision_array[1:], decision_array[:-1])\n",
    "        indeces_of_boundaries = np.where(ups_and_downs)[0]\n",
    "        maxima = []\n",
    "        boundaries = np.append(\n",
    "            np.append(np.zeros(1), indeces_of_boundaries),\n",
    "            np.zeros(1) + len(our_array),\n",
    "        )\n",
    "        # print(boundaries)\n",
    "        boundaries = boundaries.astype(np.int32)\n",
    "        for slice_start, slice_end in zip(boundaries[::2], boundaries[1::2]):\n",
    "            #print(slice_start, slice_end)\n",
    "            beat = our_array[slice_start:slice_end]\n",
    "            maxima.append(slice_start + np.where(beat == beat.max())[0][0])\n",
    "        maxima_values = our_array[maxima]\n",
    "        # print(maxima_values)\n",
    "        rep_array = np.zeros(len(our_array))\n",
    "        rep_array[maxima] = np.mean(maxima_values)\n",
    "        plt.plot(our_array, alpha = 0.7)\n",
    "        plt.plot(rep_array, alpha = 0.4)\n",
    "    else: \n",
    "        ups_and_downs = np.logical_xor(decision_array[1:], decision_array[:-1])\n",
    "        indeces_of_boundaries = np.where(ups_and_downs)[0]\n",
    "        maxima = []\n",
    "        boundaries = np.append(\n",
    "            indeces_of_boundaries,\n",
    "            np.zeros(1) + len(our_array),\n",
    "        )\n",
    "        boundaries = boundaries.astype(np.int32)\n",
    "        for slice_start, slice_end in zip(boundaries[::2], boundaries[1::2]):\n",
    "            #print(slice_start, slice_end)\n",
    "            beat = our_array[slice_start:slice_end]\n",
    "            maxima.append(slice_start + np.where(beat == beat.max())[0][0])\n",
    "        maxima_values = our_array[maxima]\n",
    "        #print(maxima_values)\n",
    "        rep_array = np.zeros(len(our_array))\n",
    "        rep_array[maxima] = np.mean(maxima_values)\n",
    "        plt.plot(our_array, alpha = 0.7)\n",
    "        plt.plot(rep_array, alpha = 0.4)\n",
    "    return maxima, maxima_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_view == 'Samples':\n",
    "    our_array = processed_data_emg[int(start):int(end)]\n",
    "    starter=int(start)\n",
    "    ender= int(end)\n",
    "else: \n",
    "    our_array = processed_data_emg[int(start_s):int(end_s)]\n",
    "    starter=int(start_s)\n",
    "    ender= int(end_s)\n",
    "\n",
    "find_maxima_in_high_entropy_area(our_array,start=starter, end=ender, decision_cutoff=decision_cutoff)\n",
    "# note the x axis will be from zero counting up but represent the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several decisions that need to be made here. Note the following:\n",
    "    \n",
    "    We are looking at maxima on the positive. Maybe we should be looking on both sides of zero? Maybe we should be lokking at an absolute value, and then find the maxima? Also note our cut-off on entropy was a bit arbitraty, and gave us an extra breath. These are decisions for the scientific side of a team that need to happen before this interface can go furhter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to decide if we do area under curve for the absolute value, or what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, is mean or half entropy sufficient? I don't think so, for some files it works but look at example file 12 from the start to 10,000. ENtropy of 3/4 max might have been better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now I can save off the info on this run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You probably want to change the name to a timestamp, and save every hour, at least. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
